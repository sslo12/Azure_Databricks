{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f61b452-234a-4ef6-8aa4-a5eec7cea3a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Librerías generales\n",
    "#=========================\n",
    "import json\n",
    "from json import dumps\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, explode, from_json, col, current_date\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
    "import datetime \n",
    "import requests\n",
    "import urllib3\n",
    "import matplotlib as mtl\n",
    "from datetime import datetime,date\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd64237b-8a72-4ff3-8e27-c48aecd84baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"Ambiente\", \"Produccion\", [\"Desarrollo\",\"Produccion\"])\n",
    "environment = dbutils.widgets.get(\"Ambiente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27841af-1e76-40de-b8ff-2bef213c5042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recupera datos de acuerdo al Ambiente de ejecución\n",
    "# ================================================\n",
    "\n",
    "if environment == \"Produccion\":\n",
    "    storage_account = \"santiag0r\"\n",
    "    catalog_gold   = \"gold-santiag0r\"\n",
    "    catalog_silver = \"silver-santiag0r\"\n",
    "    catalog_bronze = \"bronze-santiag0r\"\n",
    "    bucket_gold    = \"gold\"\n",
    "    bucket_silver  = \"silver\"\n",
    "    bucket_bronze  = \"bronze\"\n",
    "\n",
    "elif environment == \"Desarrollo\":\n",
    "    storage_account = \"santiag0r\"\n",
    "    catalog_gold   = \"gold-santiag0r\"\n",
    "    catalog_silver = \"silver-santiag0r\"\n",
    "    catalog_bronze = \"bronze-santiag0r\"\n",
    "    bucket_gold    = \"gold\"\n",
    "    bucket_silver  = \"silver\"\n",
    "    bucket_bronze  = \"bronze\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4d4dd3-a589-40b6-8647-83182949e891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema:\nId: int64\nPostId: int64\nVoteTypeId: int64\nCreationDate: timestamp[ns, tz=UTC]\nUserId: int64\nBountyAmount: uint64\n-- schema metadata --\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 975\nFilas filtradas: 3223079\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Leer el parquet directamente desde HTTPS\n",
    "url = \"https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/votes/2020.parquet\"\n",
    "df_votes = pd.read_parquet(url, engine=\"pyarrow\")\n",
    "\n",
    "# 2. Guardar como parquet temporal para Dataset\n",
    "temp_path = \"votes_temp.parquet\"\n",
    "df_votes.to_parquet(temp_path)\n",
    "\n",
    "# 3. Crear dataset desde el archivo local\n",
    "dataset_votes = ds.dataset(temp_path, format=\"parquet\")\n",
    "\n",
    "print(\"Esquema:\")\n",
    "print(dataset_votes.schema)\n",
    "\n",
    "# 4. Rango de fechas (DEBEN TENER tz=UTC y ns)\n",
    "start_date = pa.scalar(\n",
    "    datetime(2020, 1, 1),\n",
    "    type=pa.timestamp(\"ns\", tz=\"UTC\")\n",
    ")\n",
    "\n",
    "end_date = pa.scalar(\n",
    "    datetime(2020, 3, 1),\n",
    "    type=pa.timestamp(\"ns\", tz=\"UTC\")\n",
    ")\n",
    "\n",
    "# 5. Filtrado correcto (Enero + Febrero)\n",
    "votes_filtered = dataset_votes.to_table(\n",
    "    filter=(\n",
    "        (ds.field(\"CreationDate\") >= start_date) &\n",
    "        (ds.field(\"CreationDate\") < end_date)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Filas filtradas:\", votes_filtered.num_rows)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b87305-485b-40b0-899b-f353aae8e4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo guardado en Bronze: abfss://bronze@santiag0r.dfs.core.windows.net/votes/votes_2020.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1) Convertir Arrow Table → Pandas\n",
    "df_votes_pandas = votes_filtered.to_pandas()\n",
    "\n",
    "# 2) Fix: convertir uint64 → int64 (Spark no soporta uint64)\n",
    "if \"BountyAmount\" in df_votes_pandas.columns:\n",
    "    df_votes_pandas[\"BountyAmount\"] = df_votes_pandas[\"BountyAmount\"].astype(\"int64\")\n",
    "\n",
    "# 3) Crear Spark DataFrame\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_votes_spark = spark.createDataFrame(df_votes_pandas)\n",
    "\n",
    "# 4) Ruta de Bronze\n",
    "ruta_bronze = (\n",
    "    f\"abfss://{bucket_bronze}@{storage_account}.dfs.core.windows.net/\"\n",
    "    \"votes/votes_2020.parquet\"\n",
    ")\n",
    "\n",
    "# 5) Guardar\n",
    "(\n",
    "    df_votes_spark\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(ruta_bronze)\n",
    ")\n",
    "\n",
    "print(\"✅ Archivo guardado en Bronze:\", ruta_bronze)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingest",
   "widgets": {
    "Ambiente": {
     "currentValue": "Produccion",
     "nuid": "962e11f5-e2ef-4fd6-b5fe-8a2f1bdb5335",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Produccion",
      "label": null,
      "name": "Ambiente",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Desarrollo",
        "Produccion"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Produccion",
      "label": null,
      "name": "Ambiente",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "Desarrollo",
        "Produccion"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}